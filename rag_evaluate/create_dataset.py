import argparse
import os
import csv
import json
from datetime import datetime
from pathlib import Path
from typing import List, Dict
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(project_root))

from rag_opensearch.rag_opensearch import get_opensearch_rag
from rag_opensearch.llm_models import GeminiRAGModel


def load_testset_csv(csv_path: str) -> List[Dict]:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã‚€
    
    Args:
        csv_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        
    Returns:
        ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    """
    testset = []
    with open(csv_path, 'r', encoding='utf-8-sig') as f:  # BOMã‚’è‡ªå‹•çš„ã«å‡¦ç†
        reader = csv.DictReader(f)
        for row in reader:
            testset.append(row)
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±: CSVã®ã‚«ãƒ©ãƒ ã¨æœ€åˆã®è¡Œã®å†…å®¹ã‚’è¡¨ç¤º
    if testset:
        print(f"ğŸ“‹ CSVã‚«ãƒ©ãƒ : {list(testset[0].keys())}")
        print(f"ğŸ“‹ æœ€åˆã®è¡Œã®user_input: '{testset[0].get('user_input', 'N/A')}'")
    
    return testset


def run_rag_on_testset(
    index_name: str,
    testset: List[Dict],
    rag_method: str = 'knn',
    top_k: int = 5,
    llm_model = None,
    **rag_kwargs
) -> List[Dict]:
    """
    ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®å„è³ªå•ã«å¯¾ã—ã¦RAGã‚’å®Ÿè¡Œã—ã€å›ç­”ã¨æ¤œç´¢ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
    
    Args:
        testset: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
        rag_method: RAGæ¤œç´¢æ–¹æ³• ('knn', 'normalize', 'rrf')
        top_k: å–å¾—ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
        **rag_kwargs: RAGã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        
    Returns:
        å›ç­”ã¨æ¤œç´¢ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¿½åŠ ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
    """
    # RAGã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–
    rag = get_opensearch_rag(
        index_name=index_name, 
        top_k=top_k,
        llm_model=llm_model,
        **rag_kwargs
    )
    
    results = []
    total = len(testset)
    
    print(f"\n=== RAGå®Ÿè¡Œé–‹å§‹ ===")
    print(f"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆæ•°: {total}")
    print(f"RAGæ–¹æ³•: {rag_method}")
    print(f"Top-K: {top_k}\n")
    
    for idx, test_item in enumerate(testset, 1):
        question = test_item.get('user_input', '').strip()
        
        # è³ªå•ãŒç©ºã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
        if not question:
            print(f"[{idx}/{total}] âš ï¸ è­¦å‘Š: è³ªå•ãŒç©ºã§ã™ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            test_item['response'] = "ERROR: è³ªå•ãŒç©ºã§ã™"
            test_item['retrieved_contexts'] = json.dumps([], ensure_ascii=False)
            results.append(test_item)
            continue
        
        print(f"[{idx}/{total}] è³ªå•: {question[:50]}...")
        
        try:
            # RAGã§å›ç­”ã‚’å–å¾—
            result = rag.answer(question, k=top_k, verbose=False)
            
            # å›ç­”ã‚’è¿½åŠ 
            test_item['response'] = result['answer']
            
            # æ¤œç´¢ã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®contentã‚’ãƒªã‚¹ãƒˆã¨ã—ã¦è¿½åŠ 
            retrieved_contexts = [doc['content'] for doc in result['sources']]
            # JSONå½¢å¼ã§ä¿å­˜ï¼ˆCSVã«ä¿å­˜ã™ã‚‹ãŸã‚ï¼‰
            test_item['retrieved_contexts'] = json.dumps(retrieved_contexts, ensure_ascii=False)
            
            print(f"  âœ“ å®Œäº† (æ¤œç´¢ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {len(retrieved_contexts)})")
            
        except Exception as e:
            print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {e}")
            test_item['response'] = f"ERROR: {str(e)}"
            test_item['retrieved_contexts'] = json.dumps([], ensure_ascii=False)
        
        results.append(test_item)
    
    print(f"\n=== RAGå®Ÿè¡Œå®Œäº† ===\n")
    return results


def save_dataset_csv(dataset: List[Dict], output_dir: str = None):
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    
    Args:
        dataset: ä¿å­˜ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: outputs/testdatas/datasetsï¼‰
    """
    if output_dir is None:
        output_dir = project_root / "outputs" / "testdatas" / "datasets"
    else:
        output_dir = Path(output_dir)
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã«ç¾åœ¨æ™‚åˆ»ã‚’å«ã‚ã‚‹
    current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_path = output_dir / f"dataset_{current_time}.csv"
    
    # CSVã«æ›¸ãè¾¼ã¿
    if dataset:
        fieldnames = list(dataset[0].keys())
        
        with open(output_path, 'w', encoding='utf-8-sig', newline='') as f: # BOMã¤ãã§å‡ºåŠ›ã—ã€Excelã§æ–‡å­—åŒ–ã‘ã—ãªã„
            writer = csv.DictWriter(
                f, 
                fieldnames=fieldnames,
                quoting=csv.QUOTE_ALL,
                doublequote=True
            )
            writer.writeheader()
            writer.writerows(dataset)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_path}")
        print(f"   ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {len(dataset)}")
    else:
        print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç©ºã®ãŸã‚ä¿å­˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")


def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    ä½¿ç”¨ä¾‹:
        python -m src.ragas.create_dataset  --index-name tesseract-ocr
    """
    parser = argparse.ArgumentParser(description="Ragasç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    default_input_csv = project_root / "outputs" / "testdatas" / "testsets" / "testset_20251111144851.csv"
    parser.add_argument(
        "--index-name",
        required=True,
        help="OpenSearchã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å"
    )
    parser.add_argument(
        "--input-csv",
        default=str(default_input_csv),
        help="ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹"
    )

    args = parser.parse_args()

    # å…¥åŠ›CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
    input_csv_path = Path(args.input_csv)
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
    if not input_csv_path.exists():
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_csv_path}")
        print("\nåˆ©ç”¨å¯èƒ½ãªãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ:")
        testsets_dir = project_root / "outputs" / "testdatas" / "testsets"
        if testsets_dir.exists():
            for csv_file in testsets_dir.glob("*.csv"):
                print(f"  - {csv_file.name}")
        return
    
    print(f"ğŸ“„ å…¥åŠ›CSV: {input_csv_path}")
    
    # ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
    testset = load_testset_csv(str(input_csv_path))
    print(f"âœ“ ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†: {len(testset)}ä»¶\n")

    llm_model = GeminiRAGModel(
        model_name="gemini-2.5-pro",
        temperature=0.7,
        max_output_tokens=10000,
        thinking_budget=128
    )

    # RAGã‚’å®Ÿè¡Œ
    # rag_method: 'knn', 'normalize', 'rrf' ã‹ã‚‰é¸æŠ
    dataset = run_rag_on_testset(
        index_name=args.index_name,
        testset=testset,
        rag_method='rrf',  # ã“ã“ã§æ¤œç´¢æ–¹æ³•ã‚’æŒ‡å®š
        top_k=4, # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚ã‚‹ãƒãƒ£ãƒ³ã‚¯æ•°ä¸Šä½kä»¶
        llm_model=llm_model,
        ## normalize ã®å ´åˆã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¾‹:
        # knn_weight=0.7,
        # bm25_weight=0.3,
        # normalization_technique='min_max',
        # combination_technique='arithmetic_mean'
        # rrf ã®å ´åˆã®è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¾‹:
        rank_constant=60
    )
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜
    save_dataset_csv(dataset)


if __name__ == "__main__":
    main()
